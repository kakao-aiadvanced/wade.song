{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-30T06:52:29.910406Z",
     "start_time": "2024-07-30T06:52:29.019436Z"
    }
   },
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kakao/PycharmProjects/wade.song/.venv/lib/python3.8/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T06:53:58.582834Z",
     "start_time": "2024-07-30T06:53:58.252149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "               \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "               \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "for doc in docs:\n",
    "    print(doc.page_content[:100])  # 각 문서의 첫 100자를 출력"
   ],
   "id": "2daacd506728b712",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |\n",
      "\n",
      "\n",
      "      Prompt Engineering\n",
      "    \n",
      "Date: March 15, 2023  |  Estimated Reading Time: 21 min  |  Author: \n",
      "\n",
      "\n",
      "      Adversarial Attacks on LLMs\n",
      "    \n",
      "Date: October 25, 2023  |  Estimated Reading Time: 33 min  \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T06:54:58.997260Z",
     "start_time": "2024-07-30T06:54:58.991250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "splits[-10:]"
   ],
   "id": "fa107bdcfdb3c5f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'}, page_content='Inner maximization: find the most effective adversarial data point, $\\\\mathbf{x} + \\\\boldsymbol{\\\\delta}$, that leads to high loss. All the adversarial attack methods eventually come down to ways to maximize the loss in the inner loop.\\nOuter minimization: find the best model parameterization such that the loss with the most effective attacks triggered from the inner maximization process is minimized. Naive way to train a robust model is to replace each data point with their perturbed versions, which can be multiple adversarial variants of one data point.\\n\\n\\nFig. 17. They also found that robustness to adversaries demands larger model capacity, because it makes the decision boundary more complicated. Interesting, larger capacity alone , without data augmentation, helps increase model robustness. (Image source: Madry et al. 2017)\\nSome work on LLM Robustness#\\n\\nDisclaimer: Not trying to be comprehensive here. Need a separate blog post to go deeper.)'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'}, page_content='One simple and intuitive way to defend the model against adversarial attacks is to explicitly instruct model to be responsible, not generating harmful content (Xie et al. 2023). It can largely reduce the success rate of jailbreak attacks, but has side effects for general model quality due to the model acting more conservatively (e.g. for creative writing) or incorrectly interpreting the instruction under some scenarios (e.g. safe-unsafe classification).'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'}, page_content='The most common way to mitigate risks of adversarial attacks is to train the model on those attack samples, known as adversarial training. It is considered as the strongest defense but leading to tradeoff between robustness and model performance. In an experiment by Jain et al. 2023, they tested two adversarial training setups: (1) run gradient descent on harmful prompts paired with \"I\\'m sorry. As a ...\" response; (2) run one descent step on a refusal response and an ascend step on a red-team bad response per training step. The method (2) ends up being quite useless because the model generation quality degrades a lot, while the drop in attack success rate is tiny.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'}, page_content='White-box attacks often lead to nonsensical adversarial prompts and thus they can be detected by examining perplexity. Of course, a white-box attack can directly bypass this by explicitly optimizing for lower perplexity, such as UAT-LM, a variation of UAT. However, there is a tradeoff and it can lead to lower attack success rate.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'}, page_content='Fig. 18. Perplexity filter can block attacks by [Zou et al. (2023)](https://arxiv.org/abs/2307.15043). \"PPL Passed\" and \"PPL Window Passed\" are the rates at which harmful prompts with an adversarial suffix bypass the filter without detection. The lower the pass rate the better the filter is. (Image source: Jain et al. 2023)\\nJain et al. 2023 also tested methods of preprocessing text inputs to remove adversarial modifications while semantic meaning remains.\\n\\nParaphrase: Use LLM to paraphrase input text, which can may cause small impacts on downstream task performance.\\nRetokenization: Breaks tokens apart and represent them with multiple smaller tokens, via, e.g. BPE-dropout (drop random p% tokens). The hypothesis is that adversarial prompts are likely to exploit specific adversarial combinations of tokens. This does help degrade the attack success rate but is limited, e.g. 90+% down to 40%.\\n\\nCitation#\\nCited as:'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'}, page_content='Citation#\\nCited as:\\n\\nWeng, Lilian. (Oct 2023). “Adversarial Attacks on LLMs”. Lil’Log. https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'}, page_content='Or\\n@article{weng2023attack,\\n  title   = \"Adversarial Attacks on LLMs\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Oct\",\\n  url     = \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\"\\n}\\nReferences#\\n[1] Madry et al. “Towards Deep Learning Models Resistant to Adversarial Attacks”. ICLR 2018.\\n[2] Ribeiro et al. “Semantically equivalent adversarial rules for debugging NLP models”. ACL 2018.\\n[3] Guo et al. “Gradient-based adversarial attacks against text transformers”. arXiv preprint arXiv:2104.13733 (2021).\\n[4] Ebrahimi et al. “HotFlip: White-Box Adversarial Examples for Text Classification”. ACL 2018.\\n[5] Wallace et al. “Universal Adversarial Triggers for Attacking and Analyzing NLP.” EMNLP-IJCNLP 2019. | code\\n[6] Mehrabi et al. “Robust Conversational Agents against Imperceptible Toxicity Triggers.” NAACL 2022.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'}, page_content='[6] Mehrabi et al. “Robust Conversational Agents against Imperceptible Toxicity Triggers.” NAACL 2022.\\n[7] Zou et al. “Universal and Transferable Adversarial Attacks on Aligned Language Models.” arXiv preprint arXiv:2307.15043 (2023)\\n[8] Deng et al. “RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning.” EMNLP 2022.\\n[9] Jin et al. “Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment.” AAAI 2020.\\n[10] Li et al. “BERT-Attack: Adversarial Attack Against BERT Using BERT.” EMNLP 2020.\\n[11] Morris et al. \"TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP.\" EMNLP 2020.\\n[12] Xu et al. “Bot-Adversarial Dialogue for Safe Conversational Agents.” NAACL 2021.\\n[13] Ziegler et al. “Adversarial training for high-stakes reliability.” NeurIPS 2022.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'}, page_content='[12] Xu et al. “Bot-Adversarial Dialogue for Safe Conversational Agents.” NAACL 2021.\\n[13] Ziegler et al. “Adversarial training for high-stakes reliability.” NeurIPS 2022.\\n[14] Anthropic, “Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned.” arXiv preprint arXiv:2202.03286 (2022)\\n[15] Perez et al. “Red Teaming Language Models with Language Models.” arXiv preprint arXiv:2202.03286 (2022)\\n[16] Ganguli et al. “Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned.” arXiv preprint arXiv:2209.07858 (2022)\\n[17] Mehrabi et al. “FLIRT: Feedback Loop In-context Red Teaming.” arXiv preprint arXiv:2308.04265 (2023)\\n[18] Casper et al. “Explore, Establish, Exploit: Red Teaming Language Models from Scratch.” arXiv preprint arXiv:2306.09442 (2023)\\n[19] Xie et al. “Defending ChatGPT against Jailbreak Attack via Self-Reminder.” Research Square (2023)'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'}, page_content='[19] Xie et al. “Defending ChatGPT against Jailbreak Attack via Self-Reminder.” Research Square (2023)\\n[20] Jones et al. “Automatically Auditing Large Language Models via Discrete Optimization.” arXiv preprint arXiv:2303.04381 (2023)\\n[21] Greshake et al. “Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection.” arXiv preprint arXiv:2302.12173(2023)\\n[22] Jain et al. “Baseline Defenses for Adversarial Attacks Against Aligned Language Models.” arXiv preprint arXiv:2309.00614 (2023)\\n[23] Wei et al. “Jailbroken: How Does LLM Safety Training Fail?” arXiv preprint arXiv:2307.02483 (2023)\\n[24] Wei & Zou. “EDA: Easy data augmentation techniques for boosting performance on text classification tasks.”  EMNLP-IJCNLP 2019.\\n[25] www.jailbreakchat.com\\n[26] WitchBOT. “You can use GPT-4 to create prompt injections against GPT-4” Apr 2023.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T06:57:27.239015Z",
     "start_time": "2024-07-30T06:57:20.307445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ],
   "id": "17f06056522e7715",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T06:57:32.597764Z",
     "start_time": "2024-07-30T06:57:29.856259Z"
    }
   },
   "cell_type": "code",
   "source": "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))",
   "id": "5c8ef05ebace2071",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:11:17.933349Z",
     "start_time": "2024-07-30T07:11:17.466389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
    ")\n",
    "retrieved_docs = retriever.invoke(\"agent memory\")"
   ],
   "id": "d0f3768994e54d3e",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:22:15.019448Z",
     "start_time": "2024-07-30T07:22:15.017402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_docs(docs):\n",
    "    formated = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    return formated"
   ],
   "id": "d27d9b19539efb42",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:11:23.441372Z",
     "start_time": "2024-07-30T07:11:23.423530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ],
   "id": "235720e0ecbcb6c",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:10:04.560853Z",
     "start_time": "2024-07-30T08:10:04.554597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "#User query와 retrieved chunk 에 대해 relevance 가 있는지를 평가하는 시스템 프롬프트 작성\n",
    "def prompt_relevance():\n",
    "    res = '''\n",
    "**System Prompt:**\n",
    "\n",
    "You are provided with a User query and a retrieved chunk of text. Your task is to evaluate whether the retrieved chunk is relevant to the User query. \n",
    "\n",
    "1. **User Query:** \n",
    "```\n",
    "{question}\n",
    "```\n",
    "\n",
    "2. **Retrieved Chunk:** \n",
    "```\n",
    "{context}\n",
    "```\n",
    "\n",
    "\n",
    "Please analyze the retrieved chunk and determine if it is relevant to the User query. \n",
    "\n",
    "If the chunk is relevant to the query, output: \n",
    "{{\"relevance\": \"yes\"}}\n",
    "\n",
    "If the chunk is not relevant to the query, output: \n",
    "{{\"relevance\": \"no\"}}\n",
    "\n",
    "Please ensure your evaluation is based on the relevance of the content of the retrieved chunk in relation to the User query.\n",
    "    '''\n",
    "    return res\n",
    "\n",
    "\n"
   ],
   "id": "7a3627c2aa1335ce",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:13:01.507900Z",
     "start_time": "2024-07-30T08:12:57.504221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "retriever = vectorstore.as_retriever()\n",
    "retrieved_docs2 = retriever.invoke(\"What is decomposition?\")\n",
    "for retrieved_doc in retrieved_docs2:\n",
    "    prompt_relevance_res = prompt_relevance()\n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_relevance(),\n",
    "        input_variables=[\"question\", \"context\"]\n",
    "    )\n",
    "    \n",
    "\n",
    "    chain = RunnablePassthrough() | prompt | llm | JsonOutputParser()\n",
    "    result = chain.invoke({\"question\": \"What is decomposition?\", \"context\": retrieved_doc.page_content})\n",
    "    "
   ],
   "id": "5ed000909bb7b67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "{'relevance': 'no'}\n",
      "Fig. 1. Overview of a LLM-powered autonomous agent system.\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "{'relevance': 'yes'}\n",
      "Zero-shot CoT. Use natural language statement like Let's think step by step to explicitly encourage the model to first generate reasoning chains and then to prompt with Therefore, the answer is to produce answers (Kojima et al. 2022 ). Or a similar statement Let's work this out it a step by step to be sure we have the right answer (Zhou et al. 2022).\n",
      "\n",
      "Question: Marty has 100 centimeters of ribbon that he must cut into 4 equal parts. Each of the cut parts must be divided into 5 equal parts. How long will each final cut be?\n",
      "Answer: Let's think step by step.\n",
      "Tips and Extensions#\n",
      "\n",
      "\n",
      "Self-consistency sampling can improve reasoning accuracy by sampling a number of diverse answers and then taking the majority vote. (Wang et al. 2022a)\n",
      "{'relevance': 'no'}\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "Self-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\n",
      "{'relevance': 'no'}\n"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:18:27.805483Z",
     "start_time": "2024-07-30T08:18:27.797408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_relevance(question, retrieved_chunk):\n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_relevance(),\n",
    "        input_variables=[\"question\", \"context\"]\n",
    "    )\n",
    "\n",
    "\n",
    "    chain = RunnablePassthrough() | prompt | llm | JsonOutputParser()\n",
    "    result = chain.invoke({\"question\": question, \"context\": retrieved_chunk})\n",
    "    return result.get(\"relevance\") == \"yes\"\n",
    "\n",
    "def invoke_and_filter_related_docs(question):\n",
    "    retrieved_docs2 = retriever.invoke(question)\n",
    "    res = []\n",
    "    for retrieved_doc in retrieved_docs2:\n",
    "        if check_relevance(question, retrieved_doc.page_content):\n",
    "            res.append(retrieved_doc.page_content)\n",
    "    return res"
   ],
   "id": "74adeba10e90f88a",
   "outputs": [],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:18:31.532741Z",
     "start_time": "2024-07-30T08:18:28.630535Z"
    }
   },
   "cell_type": "code",
   "source": "print(invoke_and_filter_related_docs(\"What is decomposition?\"))",
   "id": "2e607800ad5bd33a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.']\n"
     ]
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:31:12.818465Z",
     "start_time": "2024-07-30T08:31:07.355971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc for doc in docs)\n",
    "\n",
    "\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# question = \n",
    "rag_chain = (\n",
    "        {\"context\": lambda question: format_docs(invoke_and_filter_related_docs(question)), \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ],
   "id": "96c5deed22492d2d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Task decomposition is the process of breaking down a complex task into smaller, manageable steps. It can be achieved through various methods, such as prompting a language model (LLM) for subgoals, using task-specific instructions, or incorporating human inputs. This approach enhances the model's performance by allowing it to think through the task step by step.\""
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 132
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "76d38db281ee97d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
